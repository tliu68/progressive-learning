{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "from math import log2, ceil\n",
    "import ardent\n",
    "# from scipy.ndimage import zoom\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from proglearn.progressive_learner import ProgressiveLearner\n",
    "from proglearn.deciders import SimpleArgmaxAverage\n",
    "from proglearn.transformers import TreeClassificationTransformer, NeuralClassificationTransformer\n",
    "from proglearn.voters import TreeClassificationVoter, KNNClassificationVoter\n",
    "# from proglearn.sims import generate_gaussian_parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "def _generate_2d_rotation(theta=0):\n",
    "    R = np.array([[np.cos(theta), np.sin(theta)], [-np.sin(theta), np.cos(theta)]])\n",
    "\n",
    "    return R\n",
    "\n",
    "\n",
    "def generate_gaussian_parity(\n",
    "    n_samples,\n",
    "    centers=None,\n",
    "    class_label=None,\n",
    "    cluster_std=0.25,\n",
    "    angle_params=None,\n",
    "    random_state=None,\n",
    "):\n",
    "\n",
    "    if random_state != None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    if centers == None:\n",
    "        centers = np.array([(-0.5, 0.5), (0.5, 0.5), (-0.5, -0.5), (0.5, -0.5)])\n",
    "\n",
    "    if class_label == None:\n",
    "        class_label = [0, 1, 1, 0]\n",
    "\n",
    "    blob_num = len(class_label)\n",
    "\n",
    "    # get the number of samples in each blob with equal probability\n",
    "    samples_per_blob = np.random.multinomial(\n",
    "        n_samples, 1 / blob_num * np.ones(blob_num)\n",
    "    )\n",
    "\n",
    "    X, y = make_blobs(\n",
    "        n_samples=samples_per_blob,\n",
    "        n_features=2,\n",
    "        centers=centers,\n",
    "        cluster_std=cluster_std,\n",
    "    )\n",
    "\n",
    "    for blob in range(blob_num):\n",
    "        y[np.where(y == blob)] = class_label[blob]\n",
    "\n",
    "    if angle_params != None:\n",
    "        R = _generate_2d_rotation(angle_params)\n",
    "        X = X @ R\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_grid(X_task):\n",
    "    h = 0.01\n",
    "    x_min, x_max = X_task[:,0].min()-0.1, X_task[:,0].max()+0.1\n",
    "    y_min, y_max = X_task[:,1].min()-0.1, X_task[:,1].max()+0.1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    new_X_task = np.c_[xx.ravel(), yy.ravel()]\n",
    "    return xx, yy, new_X_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_forest(max_depth):\n",
    "    default_transformer_class = TreeClassificationTransformer\n",
    "    default_transformer_kwargs = {\"kwargs\" : {\"max_depth\" : max_depth}}\n",
    "\n",
    "    default_voter_class = TreeClassificationVoter\n",
    "    default_voter_kwargs = {}\n",
    "\n",
    "    default_decider_class = SimpleArgmaxAverage\n",
    "    default_decider_kwargs = {\"classes\" : np.arange(2)}\n",
    "    progressive_learner = ProgressiveLearner(default_transformer_class = default_transformer_class,\n",
    "                                            default_transformer_kwargs = default_transformer_kwargs,\n",
    "                                            default_voter_class = default_voter_class,\n",
    "                                            default_voter_kwargs = default_voter_kwargs,\n",
    "                                            default_decider_class = default_decider_class,\n",
    "                                            default_decider_kwargs = default_decider_kwargs)\n",
    "    uf = ProgressiveLearner(default_transformer_class = default_transformer_class,\n",
    "                                            default_transformer_kwargs = default_transformer_kwargs,\n",
    "                                            default_voter_class = default_voter_class,\n",
    "                                            default_voter_kwargs = default_voter_kwargs,\n",
    "                                            default_decider_class = default_decider_class,\n",
    "                                            default_decider_kwargs = default_decider_kwargs)\n",
    "    return progressive_learner, uf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_no = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed_no)\n",
    "#source data\n",
    "X_task1, y_task1 = generate_gaussian_parity(100, angle_params=0)\n",
    "test_task1, test_label_task1 = generate_gaussian_parity(1000, angle_params=0)\n",
    "\n",
    "#target data\n",
    "X_task2, y_task2 = generate_gaussian_parity(100, angle_params=np.pi/4)\n",
    "test_task2, test_label_task2 = generate_gaussian_parity(1000, angle_params=np.pi/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_forest(max_depth):\n",
    "    default_transformer_class = TreeClassificationTransformer\n",
    "    default_transformer_kwargs = {\"kwargs\" : {\"max_depth\" : max_depth}}\n",
    "\n",
    "    default_voter_class = TreeClassificationVoter\n",
    "    default_voter_kwargs = {}\n",
    "\n",
    "    default_decider_class = SimpleArgmaxAverage\n",
    "    default_decider_kwargs = {\"classes\" : np.arange(2)}\n",
    "    progressive_learner = ProgressiveLearner(default_transformer_class = default_transformer_class,\n",
    "                                            default_transformer_kwargs = default_transformer_kwargs,\n",
    "                                            default_voter_class = default_voter_class,\n",
    "                                            default_voter_kwargs = default_voter_kwargs,\n",
    "                                            default_decider_class = default_decider_class,\n",
    "                                            default_decider_kwargs = default_decider_kwargs)\n",
    "    uf = ProgressiveLearner(default_transformer_class = default_transformer_class,\n",
    "                                            default_transformer_kwargs = default_transformer_kwargs,\n",
    "                                            default_voter_class = default_voter_class,\n",
    "                                            default_voter_kwargs = default_voter_kwargs,\n",
    "                                            default_decider_class = default_decider_class,\n",
    "                                            default_decider_kwargs = default_decider_kwargs)\n",
    "    return progressive_learner, uf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "progressive_learner, uf = init_forest(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = 10\n",
    "np.random.seed(seed_no)\n",
    "\n",
    "progressive_learner.add_task(X_task1, y_task1, num_transformers=n_trees)\n",
    "progressive_learner.add_task(X_task2, y_task2, num_transformers=n_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2F o lddmm\n",
    "\n",
    "np.random.seed(seed_no)\n",
    "xx1, yy1, test_task1_grid = to_grid(test_task1)\n",
    "# task1_pos1 = progressive_learner.predict(test_task1_grid, task_id=0)\n",
    "task1_pos1, task1_pos2 = progressive_learner.predict(test_task1_grid, task_id=0, registration=True)\n",
    "\n",
    "xx2, yy2, test_task2_grid = to_grid(test_task2)\n",
    "# task2_pos2 = progressive_learner.predict(test_task2_grid, task_id=0)\n",
    "task2_pos1, task2_pos2 = progressive_learner.predict(test_task2_grid, task_id=1, registration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uf\n",
    "\n",
    "np.random.seed(seed_no)\n",
    "uf.add_task(X_task1, y_task1, num_transformers=2*n_trees)\n",
    "uf.add_task(X_task2, y_task2, num_transformers=2*n_trees)\n",
    "xx1, yy1, test_task1_grid = to_grid(test_task1)\n",
    "uf_task1 = uf.predict(test_task1, transformer_ids=[0], task_id=0)\n",
    "\n",
    "xx2, yy2, test_task2_grid = to_grid(test_task2)\n",
    "uf_task2 = uf.predict(test_task2, transformer_ids=[1], task_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_to_pred(task_pos_reshaped, test_task, xx, yy):\n",
    "    pred = np.empty((len(test_task), ))\n",
    "    for i in range(len(test_task)):\n",
    "        x_ind = np.where(test_task[i,0] <= xx[0,:])[0][0]\n",
    "        y_ind = np.where(test_task[i,1] <= yy[:,0])[0][0]\n",
    "        pred[i] = task_pos_reshaped[y_ind, x_ind]  # y_ind row, x_ind column\n",
    "    pred = pred.reshape((-1,1))\n",
    "    pred = np.hstack((pred, 1-pred))\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_to_score(l2f_task, test_task, xx, yy):\n",
    "    pred = np.empty((len(test_task), ))\n",
    "    l2f_task = l2f_task.reshape(xx.shape)\n",
    "    for i in range(len(test_task)):\n",
    "        x_ind = np.where(test_task[i,0] <= xx[0,:])[0][0]\n",
    "        y_ind = np.where(test_task[i,1] <= yy[:,0])[0][0]\n",
    "        pred[i] = l2f_task[x_ind, y_ind]\n",
    "    # pred = pred.reshape((1,-1))\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_pos1_reshaped = task1_pos1[:,0].reshape(xx1.shape)\n",
    "task1_pos2_reshaped = task1_pos2[:,0].reshape(xx1.shape)\n",
    "task2_pos1_reshaped = task2_pos1[:,0].reshape(xx2.shape)\n",
    "task2_pos2_reshaped = task2_pos2[:,0].reshape(xx2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lddmm_reg(in_task_pos_reshaped, cross_task_pos_reshaped):\n",
    "    print(\"start running lddmm\")\n",
    "    transform = ardent.Transform()\n",
    "    reference = in_task_pos_reshaped\n",
    "    moving = cross_task_pos_reshaped\n",
    "\n",
    "    try:\n",
    "        transform.register(target=moving, template=reference)\n",
    "    except RuntimeError:\n",
    "        try:\n",
    "            transform.register(target=moving, template=reference, affine_stepsize=0.2)\n",
    "        except RuntimeError:\n",
    "            try:\n",
    "                transform.register(target=moving, template=reference, affine_stepsize=0.1)\n",
    "            except RuntimeError:\n",
    "                try:\n",
    "                    transform.register(target=moving, template=reference, affine_stepsize=0.05)\n",
    "                except RuntimeError:\n",
    "                    return moving\n",
    "\n",
    "    deformed_moving = transform.transform_image(\n",
    "        subject=moving,\n",
    "        output_shape=moving.shape,\n",
    "        deform_to='template')\n",
    "    print(\"finish running lddmm\")\n",
    "    return deformed_moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.zeros(6, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote0 = np.mean([task1_pos1_reshaped, task1_pos2_reshaped], axis=0)\n",
    "errors[0] = 1 - np.mean(grid_to_pred(vote0, test_task1, xx1, yy1) == test_label_task1)  # l2f_task1_error\n",
    "vote1 = np.mean([task2_pos1_reshaped, task2_pos2_reshaped], axis=0)\n",
    "errors[1] = 1 - np.mean(grid_to_pred(vote1, test_task2, xx2, yy2) == test_label_task2)  # l2f_task2_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.083, 0.079, 0.   , 0.   , 0.   , 0.   ])"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "start running lddmm\nfinish running lddmm\nstart running lddmm\nfinish running lddmm\n--- 314.81873202323914 seconds ---\n"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "task1_pos2_deformed = lddmm_reg(task1_pos1_reshaped, task1_pos2_reshaped)\n",
    "vote2 = np.mean([task1_pos1_reshaped, task1_pos2_deformed], axis=0)\n",
    "errors[2] = 1 - np.mean(grid_to_pred(vote2, test_task1, xx1, yy1) == test_label_task1)  # l2f_lddmm_task1_error\n",
    "task2_pos1_deformed = lddmm_reg(task2_pos2_reshaped, task2_pos1_reshaped)\n",
    "vote3 = np.mean([task2_pos2_reshaped, task2_pos1_deformed], axis=0)\n",
    "errors[3] = 1 - np.mean(grid_to_pred(vote3, test_task2, xx2, yy2) == test_label_task2)  # l2f_lddmm_task2_error\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors[4] = 1 - np.mean(uf_task1 == test_label_task1)  # uf_task1_error\n",
    "errors[5] = 1 - np.mean(uf_task2 == test_label_task2)  # uf_task1_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.083, 0.079, 0.074, 0.082, 0.081, 0.091])"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp(n_task1, n_task2, n_test=1000, task1_angle=0, task2_angle=np.pi/4, n_trees=10, rand=None):\n",
    "    if rand is not None:\n",
    "        np.random.seed(rand)\n",
    "    progressive_learner, uf = init_forest(max_depth=ceil(log2(n_task1)))\n",
    "    errors = np.zeros(6, dtype=float)\n",
    "\n",
    "    # np.random.seed(rand)\n",
    "    #source data\n",
    "    X_task1, y_task1 = generate_gaussian_parity(n_task1, angle_params=task1_angle)\n",
    "    test_task1, test_label_task1 = generate_gaussian_parity(n_test, angle_params=task1_angle)\n",
    "    #target data\n",
    "    X_task2, y_task2 = generate_gaussian_parity(n_task2, angle_params=task2_angle)\n",
    "    test_task2, test_label_task2 = generate_gaussian_parity(n_test, angle_params=task2_angle)\n",
    "\n",
    "    # np.random.seed(rand)\n",
    "    # l2f\n",
    "    progressive_learner.add_task(X_task1, y_task1, num_transformers=n_trees)\n",
    "    progressive_learner.add_task(X_task2, y_task2, num_transformers=n_trees)\n",
    "\n",
    "    xx1, yy1, test_task1_grid = to_grid(test_task1)\n",
    "    task1_pos1, task1_pos2 = progressive_learner.predict(test_task1_grid, task_id=0, registration=True)\n",
    "    xx2, yy2, test_task2_grid = to_grid(test_task2)\n",
    "    task2_pos1, task2_pos2 = progressive_learner.predict(test_task2_grid, task_id=1, registration=True)\n",
    "\n",
    "    task1_pos1_reshaped = task1_pos1[:,0].reshape(xx1.shape)\n",
    "    task1_pos2_reshaped = task1_pos2[:,0].reshape(xx1.shape)\n",
    "    task2_pos1_reshaped = task2_pos1[:,0].reshape(xx2.shape)\n",
    "    task2_pos2_reshaped = task2_pos2[:,0].reshape(xx2.shape)\n",
    "\n",
    "    vote0 = np.mean([task1_pos1_reshaped, task1_pos2_reshaped], axis=0)\n",
    "    errors[0] = 1 - np.mean(grid_to_pred(vote0, test_task1, xx1, yy1) == test_label_task1)  # l2f_task1_error\n",
    "    vote1 = np.mean([task2_pos1_reshaped, task2_pos2_reshaped], axis=0)\n",
    "    errors[1] = 1 - np.mean(grid_to_pred(vote1, test_task2, xx2, yy2) == test_label_task2)  # l2f_task2_error\n",
    "\n",
    "    # np.random.seed(rand)\n",
    "    # lddmm\n",
    "    task1_pos2_deformed = lddmm_reg(task1_pos1_reshaped, task1_pos2_reshaped)\n",
    "    vote2 = np.mean([task1_pos1_reshaped, task1_pos2_deformed], axis=0)\n",
    "    errors[2] = 1 - np.mean(grid_to_pred(vote2, test_task1, xx1, yy1) == test_label_task1)  # l2f_lddmm_task1_error\n",
    "    task2_pos1_deformed = lddmm_reg(task2_pos2_reshaped, task2_pos1_reshaped)\n",
    "    vote3 = np.mean([task2_pos2_reshaped, task2_pos1_deformed], axis=0)\n",
    "    errors[3] = 1 - np.mean(grid_to_pred(vote3, test_task2, xx2, yy2) == test_label_task2)  # l2f_lddmm_task2_error\n",
    "\n",
    "\n",
    "    # uf\n",
    "    uf.add_task(X_task1, y_task1, num_transformers=2*n_trees)\n",
    "    uf.add_task(X_task2, y_task2, num_transformers=2*n_trees)\n",
    "\n",
    "    uf_task1 = uf.predict(test_task1, transformer_ids=[0], task_id=0)\n",
    "    uf_task2 = uf.predict(test_task2, transformer_ids=[1], task_id=1)\n",
    "\n",
    "    errors[4] = 1 - np.mean(uf_task1 == test_label_task1)  # uf_task1_error\n",
    "    errors[5] = 1 - np.mean(uf_task2 == test_label_task2)  # uf_task1_error\n",
    "\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# errors = exp(100, 100)\n",
    "mc_rep = 10\n",
    "# name the set of errors as error_ntask1_ntask2_frac_rep (rotation_param = np.pi/frac)\n",
    "error_100_100_4_3 = np.array(\n",
    "        Parallel(n_jobs=-1,verbose=2)(\n",
    "        delayed(exp)(n_task1=100, n_task2=100) for _ in range(mc_rep)\n",
    "      )\n",
    "    )\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37064bitproglufconda2f416a67a7dd4643b4eb9d687d02740e",
   "display_name": "Python 3.7.0 64-bit ('ProgL_UF': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}